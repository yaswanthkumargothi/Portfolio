# -*- coding: utf-8 -*-
"""review_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ay1Xb1ngT2ydYpVthy2e4FIDF7Ty4ufT

Note: You need Kaggle API key to download Kaggle datasets   
Hint: Download json file from your kaggle account and save it in kaggle folder
"""

# import libraries
import fastai
from fastai import *
from fastai.text import * 
from functools import partial
import io
import numpy as np
import pandas as pd
import os
from fastai.text import *
import html
import pickle
import re
import matplotlib.cm as cm
import sklearn
import matplotlib
from platform import python_version
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, \
confusion_matrix
from sklearn.model_selection import train_test_split
from time import time

#version check 
print(matplotlib.__version__)
print(fastai.__version__)
print(np.__version__)
print(pd.__version__)
print(sklearn.__version__)
print(python_version())

#install kaggle for accessing data
!pip install kaggle --upgrade

#make a kaggle directory in which we download .json kaggle key file
!mkdir -p ~/.kaggle/

#download .json file and move to the kaggle directory
!mv kaggle.json ~/.kaggle/

#make a amazon directory to store downloaded data
!mkdir -p ~/Amazon

#change path variable to Amazon directory
path = Config.data_path()/'Amazon'
#path.mkdir(exist_ok=True)
path

#download train file
!kaggle datasets download -d bittlingmayer/amazonreviews -f 'train.ft.txt.bz2' -p {path}

#download test file
!kaggle datasets download -d bittlingmayer/amazonreviews -f 'train.ft.txt.bz2' -p {path}

!kaggle datasets download -d bittlingmayer/amazonreviews -f 'test.ft.txt.bz2' -p {path}

!ls {path}

#install bzip2 to unzip bzip2 files
#!sudo install -c anaconda bzip2 
!sudo apt-get install bzip2

#unzip files
!unzip -q -n  /root/.fastai/data/Amazon/1305%2F800230%2Fcompressed%2Ftrain.ft.txt.bz2.zip -d {path}

#unzipping the test file
!unzip -q -n  /root/.fastai/data/Amazon/1305%2F800230%2Fcompressed%2Ftest.ft.txt.bz2.zip -d {path}

!bzip2 /root/.fastai/data/Amazon/train.ft.txt.bz2 -d {path}

!bzip2 /root/.fastai/data/Amazon/test.ft.txt.bz2 -d {path}

#convert data format as needed
train = []
with open(os.path.join(path, 'train.ft.txt'), 'r') as file:
    for line in file:
        train.append(file.readline())

test = []
with open(os.path.join(path, 'test.ft.txt'), 'r') as file:
   for line in file:
        test.append(file.readline())

print(f'The train data contains {len(train)} examples')
print(f'The test data contains {len(test)} examples')

BOS = 'xbos'  # beginning-of-sentence tag
FLD = 'xfld'  # data field tag

PATH=Path('/root/.fastai/data/Amazon/')

CLAS_PATH=PATH/'amazon_class'
CLAS_PATH.mkdir(exist_ok=True)

LM_PATH=PATH/'amazon_lm'
LM_PATH.mkdir(exist_ok=True)

#scraping labels and converting them to required format
trn_texts,trn_labels = [text[10:] for text in train], [text[:10] for text in train]
trn_labels = [0 if label == '__label__1' else 1 for label in trn_labels]
val_texts,val_labels = [text[10:] for text in test], [text[:10] for text in test]
val_labels = [0 if label == '__label__1' else 1 for label in val_labels]

#creating dataframes from labels and text
col_names = ['labels','text']

df_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, columns=col_names)
df_val = pd.DataFrame({'text':val_texts, 'labels':val_labels}, columns=col_names)

#truncating the data to fit in memory
df_trn_trunc=df_trn.iloc[1:900000,:]

df_val_trunc=df_val.iloc[1:100000,:]

df_trn_trunc.to_csv(CLAS_PATH/'train.csv', header=False, index=False)
df_val_trunc.to_csv(CLAS_PATH/'test.csv', header=False, index=False)

#Creating data bunches for language model and classifier
#language model databunch
data_lm = TextLMDataBunch.from_csv(CLAS_PATH,'train.csv')
#classifier model databunch
data_clas = TextClasDataBunch.from_csv(CLAS_PATH,'train.csv', vocab=data_lm.train_ds.vocab, bs=32)

#learning language model  from data bunches
learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)
learn.fit_one_cycle(1, 1e-2)
learn.unfreeze()
learn.fit_one_cycle(1, 1e-3)
# saving the trained encoder weights
learn.save_encoder('ft_enc')

#initializing classifier learner 
learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)

#loading saved encoder weights 
learn.load_encoder('ft_enc')
learn.fit_one_cycle(1, 1e-2)
learn.unfreeze()
learn.fit_one_cycle(1, slice(2e-3/100, 2e-3))

data_clas.show_batch()

learn.predict(data[2])

!ls '/root/.fastai/data/Amazon/amazon_class/'

!ls {path}
# saving trained weights for predictions
learn.export('/content/gdrive/My Drive/patents/export.pkl')